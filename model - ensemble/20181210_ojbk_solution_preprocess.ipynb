{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load libraries, define our train/test split, and load the word2vec dictionary using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "work_dir = '/Users/ronghao/Mirror/Cornell-Tech/2018-Fa-Course/CS-5785/Homework/Final/'\n",
    "data_dir = work_dir + 'all/'\n",
    "processed_data_dir = work_dir + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(work_dir +\\\n",
    "                            \"library/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 8000; num_dev = 2000; num_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data = 'only_fc', 'only_pool', 'fc_pool'\n",
    "# Y_data = 'full_w2v_mean', 'n_full_w2v_mean', 'a_full_w2v_mean', 'v_full_w2v_mean',\n",
    "#                           'n_top800_w2v_mean', 'a_top400_w2v_mean', 'v_top400_w2v_mean',\n",
    "#                           'n_top400_w2v_mean', 'a_top200_w2v_mean', 'v_top200_w2v_mean',\n",
    "#          'full_bow', 'n_full_bow', 'a_full_bow', 'v_full_bow',\n",
    "#                      'n_top800_bow', 'a_top400_bow', 'v_top400_bow',\n",
    "#                      'n_top400_bow', 'a_top200_bow', 'v_top200_bow',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse the ResNet features to form the X matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "X_dict = {}\n",
    "\n",
    "# build x matrices\n",
    "X_dict['train_dev'] = {}\n",
    "X_dict['test'] = {}\n",
    "\n",
    "X_dict['train_dev']['only_fc'] = parse_features(data_dir + \"features_train/features_resnet1000_train.csv\")\n",
    "X_dict['test']['only_fc'] = parse_features(data_dir + \"features_test/features_resnet1000_test.csv\")\n",
    "X_dict['train_dev']['only_pool'] = parse_features(data_dir + \"features_train/features_resnet1000intermediate_train.csv\")\n",
    "X_dict['test']['only_pool'] = parse_features(data_dir + \"features_test/features_resnet1000intermediate_test.csv\")\n",
    "X_dict['train_dev']['fc_pool'] = np.concatenate((X_dict['train_dev']['only_fc'], X_dict['train_dev']['only_pool']), axis=1)\n",
    "X_dict['test']['fc_pool'] = np.concatenate((X_dict['test']['only_fc'], X_dict['test']['only_pool']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_dict, open(work_dir + \"modeldata/X_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data = 'only_fc', 'only_pool', 'fc_pool'\n",
    "# Y_data = 'full_w2v_mean', 'n_full_w2v_mean', 'a_full_w2v_mean', 'v_full_w2v_mean',\n",
    "#                           'n_top800_w2v_mean', 'a_top400_w2v_mean', 'v_top400_w2v_mean',\n",
    "#                           'n_top400_w2v_mean', 'a_top200_w2v_mean', 'v_top200_w2v_mean',\n",
    "#          'full_bow', 'n_full_bow', 'a_full_bow', 'v_full_bow',\n",
    "#                      'n_top800_bow', 'a_top400_bow', 'v_top400_bow',\n",
    "#                      'n_top400_bow', 'a_top200_bow', 'v_top200_bow',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parse the descriptions to form the Y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dict = {}; Y_dict['train_dev'] = {}; Y_dict['test'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v on full description\n",
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    if len(sentence) == 0:\n",
    "        return np.zeros(300)\n",
    "    if type(sentence) == str:\n",
    "        word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    else:\n",
    "        word_vecs = [word2vec.get_vector(w) for w in sentence if w in word2vec.vocab]\n",
    "    # return average\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.stack(word_vecs).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full words\n",
    "train_dev_desc = parse_descriptions(data_dir + \"descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(data_dir + \"descriptions_test\", num_doc=num_test)\n",
    "\n",
    "Y_dict['train_dev']['full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc])\n",
    "Y_dict['test']['full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dict['train_dev']['full_bow'] = np.array(pickle.load(open(processed_data_dir + \"n_a_v_bow/y_train_all.p\", \"rb\")))\n",
    "Y_dict['test']['full_bow'] = np.array(pickle.load(open(processed_data_dir + \"n_a_v_bow/y_test_all.p\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads seperated word list file\n",
    "train_dev_desc_n = pickle.load(open(processed_data_dir + \"n_a_v_list/noun_list_train.p\", \"rb\"))\n",
    "test_desc_n = pickle.load(open(processed_data_dir + \"n_a_v_list/noun_list_test.p\", \"rb\"))\n",
    "train_dev_desc_a = pickle.load(open(processed_data_dir + \"n_a_v_list/adj_list_train.p\", \"rb\"))\n",
    "test_desc_a = pickle.load(open(processed_data_dir + \"n_a_v_list/adj_list_test.p\", \"rb\"))\n",
    "train_dev_desc_v = pickle.load(open(processed_data_dir + \"n_a_v_list/verb_list_train.p\", \"rb\"))\n",
    "test_desc_v = pickle.load(open(processed_data_dir + \"n_a_v_list/verb_list_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pickle.load(open(processed_data_dir + \"taglist.p\", \"rb\"))\n",
    "taglist = []\n",
    "for word in t:\n",
    "    taglist += word.split(' ')\n",
    "len(taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_feature(train_dev_desc, test_desc, vocabulary):\n",
    "    train_dev_counters = []\n",
    "    for doc in train_dev_desc:\n",
    "        c = Counter()\n",
    "        for word in doc:\n",
    "            if word in vocabulary:\n",
    "                c[word] += 1\n",
    "        train_dev_counters.append(c)\n",
    "\n",
    "    test_counters = []\n",
    "    for doc in test_desc:\n",
    "        c = Counter()\n",
    "        for word in doc:\n",
    "            if word in vocabulary:\n",
    "                c[word] += 1\n",
    "        test_counters.append(c)\n",
    "\n",
    "    table = []\n",
    "    for c in train_dev_counters:\n",
    "        row = []\n",
    "        for word in vocabulary:\n",
    "            row.append(c[word])\n",
    "        table.append(row)\n",
    "    y_train_dev = np.array(table)\n",
    "\n",
    "    table = []\n",
    "    for c in test_counters:\n",
    "        row = []\n",
    "        for word in vocabulary:\n",
    "            row.append(c[word])\n",
    "        table.append(row)\n",
    "    y_test = np.array(table)\n",
    "    \n",
    "    return y_train_dev, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_desc(train_dev_desc_origin, vocab_top):\n",
    "    train_dev_desc_n_top = []\n",
    "    for d in train_dev_desc_origin:\n",
    "        doc = [word for word in d if word in vocab_top]\n",
    "        train_dev_desc_n_top.append(doc)\n",
    "    return train_dev_desc_n_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun words\n",
    "flatten_train_dev_n = [item for sublist in train_dev_desc_n for item in sublist]\n",
    "flatten_test_n = [item for sublist in test_desc_n for item in sublist]\n",
    "n_vocab = Counter()\n",
    "for word in (flatten_train_dev_n + flatten_test_n):\n",
    "    n_vocab[word] += 1\n",
    "\n",
    "n_full_vocab = list(n_vocab.keys())\n",
    "n_800_vocab = list(t[0] for t in n_vocab.most_common(772))\n",
    "n_800_vocab = list(set.union(set(taglist), set(n_800_vocab)))\n",
    "n_400_vocab = list(t[0] for t in n_vocab.most_common(367))\n",
    "n_400_vocab = list(set.union(set(taglist), set(n_400_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun bag of words feature\n",
    "Y_dict['train_dev']['n_full_bow'], Y_dict['test']['n_full_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_n, test_desc_n, n_full_vocab)\n",
    "Y_dict['train_dev']['n_top800_bow'], Y_dict['test']['n_top800_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_n, test_desc_n, n_800_vocab)\n",
    "Y_dict['train_dev']['n_top400_bow'], Y_dict['test']['n_top400_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_n, test_desc_n, n_400_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun word2vector feature\n",
    "train_dev_desc_n_top800 = top_words_desc(train_dev_desc_n, n_800_vocab)\n",
    "train_dev_desc_n_top400 = top_words_desc(train_dev_desc_n, n_400_vocab)\n",
    "test_desc_n_top800 = top_words_desc(test_desc_n, n_800_vocab)\n",
    "test_desc_n_top400 = top_words_desc(test_desc_n, n_400_vocab)\n",
    "\n",
    "Y_dict['train_dev']['n_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_n])\n",
    "Y_dict['test']['n_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_n])\n",
    "Y_dict['train_dev']['n_top800_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_n_top800])\n",
    "Y_dict['test']['n_top800_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_n_top800])\n",
    "Y_dict['train_dev']['n_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_n_top400])\n",
    "Y_dict['test']['n_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_n_top400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjective words\n",
    "flatten_train_dev_a = [item for sublist in train_dev_desc_a for item in sublist]\n",
    "flatten_test_a = [item for sublist in test_desc_a for item in sublist]\n",
    "a_vocab = Counter()\n",
    "for word in (flatten_train_dev_a + flatten_test_a):\n",
    "    a_vocab[word] += 1\n",
    "    \n",
    "a_full_vocab = list(a_vocab.keys())\n",
    "a_400_vocab = list(t[0] for t in a_vocab.most_common(400))\n",
    "a_200_vocab = list(t[0] for t in a_vocab.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjective bag of words feature\n",
    "Y_dict['train_dev']['a_full_bow'], Y_dict['test']['a_full_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_a, test_desc_a, a_full_vocab)\n",
    "Y_dict['train_dev']['a_top400_bow'], Y_dict['test']['a_top400_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_a, test_desc_a, a_400_vocab)\n",
    "Y_dict['train_dev']['a_top200_bow'], Y_dict['test']['a_top200_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_a, test_desc_a, a_200_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjective word2vector feature\n",
    "train_dev_desc_a_top400 = top_words_desc(train_dev_desc_a, a_400_vocab)\n",
    "train_dev_desc_a_top200 = top_words_desc(train_dev_desc_a, a_200_vocab)\n",
    "test_desc_a_top400 = top_words_desc(test_desc_a, a_400_vocab)\n",
    "test_desc_a_top200 = top_words_desc(test_desc_a, a_200_vocab)\n",
    "\n",
    "Y_dict['train_dev']['a_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_a])\n",
    "Y_dict['test']['a_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_a])\n",
    "Y_dict['train_dev']['a_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_a_top400])\n",
    "Y_dict['test']['a_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_a_top400])\n",
    "Y_dict['train_dev']['a_top200_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_a_top200])\n",
    "Y_dict['test']['a_top200_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_a_top200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb words\n",
    "flatten_train_dev_v = [item for sublist in train_dev_desc_v for item in sublist]\n",
    "flatten_test_v = [item for sublist in test_desc_v for item in sublist]\n",
    "v_vocab = Counter()\n",
    "for word in (flatten_train_dev_v + flatten_test_v):\n",
    "    v_vocab[word] += 1\n",
    "    \n",
    "v_full_vocab = list(v_vocab.keys())\n",
    "v_400_vocab = list(t[0] for t in v_vocab.most_common(400))\n",
    "v_200_vocab = list(t[0] for t in v_vocab.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb bag of words feature\n",
    "Y_dict['train_dev']['v_full_bow'], Y_dict['test']['v_full_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_v, test_desc_v, v_full_vocab)\n",
    "Y_dict['train_dev']['v_top400_bow'], Y_dict['test']['v_top400_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_v, test_desc_v, v_400_vocab)\n",
    "Y_dict['train_dev']['v_top200_bow'], Y_dict['test']['v_top200_bow'] =\\\n",
    "    bag_of_words_feature(train_dev_desc_v, test_desc_v, v_200_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb word2vector feature\n",
    "train_dev_desc_v_top400 = top_words_desc(train_dev_desc_v, v_400_vocab)\n",
    "train_dev_desc_v_top200 = top_words_desc(train_dev_desc_v, v_200_vocab)\n",
    "test_desc_v_top400 = top_words_desc(test_desc_v, v_400_vocab)\n",
    "test_desc_v_top200 = top_words_desc(test_desc_v, v_200_vocab)\n",
    "\n",
    "Y_dict['train_dev']['v_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_v])\n",
    "Y_dict['test']['v_full_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_v])\n",
    "Y_dict['train_dev']['v_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_v_top400])\n",
    "Y_dict['test']['v_top400_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_v_top400])\n",
    "Y_dict['train_dev']['v_top200_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in train_dev_desc_v_top200])\n",
    "Y_dict['test']['v_top200_w2v_mean'] = np.array([doc_to_vec(d, word2vec) for d in test_desc_v_top200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Y_dict, open(work_dir + \"modeldata/Y_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute discount table\n",
    "* on dev set of different random split (rand_state=[0,9])\n",
    "* on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_discount(desc_file, tag_file):\n",
    "    table = []\n",
    "    for desc in desc_file:\n",
    "        row = []\n",
    "        for tags in tag_file:\n",
    "            d_expo = 0\n",
    "            for tag in tags:\n",
    "                if tag in desc:\n",
    "                    d_expo += 1\n",
    "            row.append(d_expo)\n",
    "        table.append(row)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_table(expo_table, exp_rate=0.8):\n",
    "    table = []\n",
    "    for expo_row in expo_table:\n",
    "        row = []\n",
    "        for expo in expo_row:\n",
    "            row.append(exp_rate**expo)\n",
    "        table.append(row)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dev Discount on different split\n",
    "num_train = 8000; num_dev = 2000; num_test = 2000\n",
    "split_idx = {}\n",
    "for rand_state in range(10):\n",
    "    split_idx[rand_state] = list(range(num_train + num_dev))\n",
    "    split_idx[rand_state] = shuffle(split_idx[rand_state], random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_file = {}\n",
    "tag_file = {}\n",
    "\n",
    "for rand_state in range(10):\n",
    "    desc_file[rand_state] = []\n",
    "    for desc_file_index in split_idx[rand_state][num_train:]:\n",
    "        # find description\n",
    "        with open(data_dir+'descriptions_train/'+str(desc_file_index)+'.txt') as f:\n",
    "            desc_file[rand_state].append(f.read().lower().\\\n",
    "                                         replace(',',' ').replace('.',' ').replace('\\n','').split(' '))\n",
    "\n",
    "    tag_file[rand_state] = []\n",
    "    for tag_file_index in split_idx[rand_state][num_train:]:\n",
    "        with open(data_dir+'tags_train/'+str(tag_file_index)+'.txt') as f:\n",
    "            tag_txt = f.read()\n",
    "        tags = []\n",
    "        for word in list(pair[pair.find(':')+1:] for pair in tag_txt.split('\\n')[:-1]):\n",
    "            tags += word.split(' ')\n",
    "        tag_file[rand_state].append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_discount_dev = {}\n",
    "for rand_state in range(10):\n",
    "    discount_expo_table = tag_discount(desc_file[rand_state], tag_file[rand_state])\n",
    "    dist_discount_dev[rand_state] = np.array(discount_table(discount_expo_table, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Test Discount\n",
    "desc_file_test = []\n",
    "for desc_file_index in range(2000):\n",
    "    # find description\n",
    "    with open(data_dir+'descriptions_test/'+str(desc_file_index)+'.txt') as f:\n",
    "        desc_file_test.append(f.read().lower().replace(',',' ').replace('.',' ').replace('\\n','').split(' '))\n",
    "\n",
    "tag_file_test = []\n",
    "for tag_file_index in range(2000):\n",
    "    with open(data_dir+'tags_test/'+str(tag_file_index)+'.txt') as f:\n",
    "        tag_txt = f.read()\n",
    "    tags = []\n",
    "    for word in list(pair[pair.find(':')+1:] for pair in tag_txt.split('\\n')[:-1]):\n",
    "        tags += word.split(' ')\n",
    "    tag_file_test.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_expo_table_test = tag_discount(desc_file_test, tag_file_test)\n",
    "dist_discount_test = np.array(discount_table(discount_expo_table_test, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dist_discount_dev, open(work_dir + \"modeldata/dist_discount_dev_dict.p\", \"wb\"))\n",
    "pickle.dump(dist_discount_test, open(work_dir + \"modeldata/dist_discount_test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save or load current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# # train PLSRegression model with regression\n",
    "# parameters = {\"n_components\": [10, 20, 30, 40]}\n",
    "# PLSreg = GridSearchCV(PLSRegression(), parameters, cv=10)\n",
    "# PLSreg.fit(x_train, y_train)\n",
    "# PLSreg_best = PLSreg.best_estimator_\n",
    "\n",
    "# print(\"Trained PLS regression model!\")\n",
    "# print(\"Summary of best model:\")\n",
    "# print(PLSreg_best)\n",
    "\n",
    "# BOW tranformed description feature\n",
    "# Y_dict['bow_1294'] = {}\n",
    "# Y_dict['bow_1294']['train_dev'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW1294/y_train.p\", \"rb\")))\n",
    "# Y_dict['bow_1294']['test'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW1294/y_test.p\", \"rb\")))\n",
    "\n",
    "# Y_dict['BOW_tagEnhanced_1'] = {}\n",
    "# Y_dict['BOW_tagEnhanced_1']['train_dev'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_train_v1.p\", \"rb\")))\n",
    "# Y_dict['BOW_tagEnhanced_1']['test'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_test_v1.p\", \"rb\")))\n",
    "\n",
    "# Y_dict['BOW_tagEnhanced_2'] = {}\n",
    "# Y_dict['BOW_tagEnhanced_2']['train_dev'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_train_v2.p\", \"rb\")))\n",
    "# Y_dict['BOW_tagEnhanced_2']['test'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_test_v2.p\", \"rb\")))\n",
    "\n",
    "# Y_dict['BOW_tagEnhanced_5'] = {}\n",
    "# Y_dict['BOW_tagEnhanced_5']['train_dev'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_train_v5.p\", \"rb\")))\n",
    "# Y_dict['BOW_tagEnhanced_5']['test'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW_tagEnhanced/y_test_v5.p\", \"rb\")))\n",
    "\n",
    "# Y_dict['bow_4291'] = {}\n",
    "# Y_dict['bow_4291']['train_dev'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW4291/y_train_v3.p\", \"rb\")))\n",
    "# Y_dict['bow_4291']['test'] = np.array(pickle.\\\n",
    "#                     load(open(processed_data_dir + \"BOW4291/y_test_v3.p\", \"rb\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
