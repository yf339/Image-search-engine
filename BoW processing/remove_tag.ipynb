{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import copy\n",
    "from nltk import Tree\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_type = ['NN', 'NNS', 'JJ', 'VB', 'VBD', 'VBP', 'VBZ', 'VBN', 'VBG', 'JJR', 'JJS']\n",
    "select_type = ['JJ','JJR', 'JJS' ]\n",
    "letter_filter = re.compile('[^a-zA-Z]')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "useless_word = set(['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\\\n",
    "                  'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\\\n",
    "                  'z', 'as', 'an', 'and', 'are', 'were', 'at', 'by', 'down', 'for', 'from',\\\n",
    "                  'front', 'has', 'in', 'is', 'it', 'near', 'next', 'of', 'on', 'out',\\\n",
    "                  'over', 'several', 'sits', 'some', 'that', 'the', 'there', 'through',\\\n",
    "                  'to', 'top', 'up', 'while', 'with', 'do'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_descrption_files = [os.path.join('data/descriptions_train', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_tag_files = [os.path.join('data/tags_train', filename)\\\n",
    "                      for filename in os.listdir('data/tags_train')\\\n",
    "                      if filename.endswith('.txt')]\n",
    "test_descrption_files = [os.path.join('data/descriptions_test', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "training_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(a):\n",
    "    return [i for i in a if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_type(s, type_select):\n",
    "    dictionary = set()\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(s)]\n",
    "    sentences = [[letter_filter.sub('', sentences[i][j].lower())\\\n",
    "              for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
    "    sentences = [remove_empty(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for sent in sentences:\n",
    "        for e in sent:\n",
    "            if e[1] in type_select:\n",
    "                dictionary.add(snowball_stemmer.stem(e[0]))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "for file in test_descrption_files:\n",
    "    aaa = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        nvj = find_type(sent, select_type)\n",
    "        for word in nvj:\n",
    "            if word not in aaa and word not in useless_word:\n",
    "                aaa.append(word)\n",
    "    adj.append(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_des_tag_vector():\n",
    "    selected = {}\n",
    "    for file in training_descrption_files:\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            nvj = find_type(sent, select_type)\n",
    "            for word in nvj:\n",
    "                if word in selected.keys():\n",
    "                    selected[word] += 1\n",
    "                else:\n",
    "                    selected[word] = 1\n",
    "    \n",
    "    final = []\n",
    "    for k in selected.keys():\n",
    "        if selected[k] >= 2 and k not in final:\n",
    "            final.append(k)\n",
    "    \n",
    "    final_copy = copy.deepcopy(final)\n",
    "    for element in final:\n",
    "        if element in useless_word:\n",
    "            final_copy.remove(element)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = define_des_tag_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3995"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_files = [os.path.join('data/tags_train', filename) \\\n",
    "                             for filename in os.listdir('data/tags_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "test_tag_files = [os.path.join('data/tags_test', filename) \\\n",
    "                             for filename in os.listdir('data/tags_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "train_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training = []\n",
    "for file in training_descrption_files:\n",
    "    vector_1 = dict.fromkeys(a, 0)\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            i = letter_filter.sub('', word.lower())\n",
    "            if i in a:\n",
    "                vector_1[i] = 1\n",
    "  \n",
    "    mm = [vector_1[element] for element in a]\n",
    "    Y_training.append(list(vector_1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "for file in test_descrption_files:\n",
    "    vector_1 = dict.fromkeys(a, 0)\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            i = letter_filter.sub('', word.lower())\n",
    "            if i in a:\n",
    "                vector_1[i] = 1\n",
    "\n",
    "    mm = [vector_1[element] for element in a]\n",
    "    Y_test.append(list(vector_1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( Y_training, open( \"y_train_v6.p\", \"wb\" ) )\n",
    "pickle.dump( Y_test, open( \"y_test_v6.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( adj, open( \"adj_list_test.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this paper, we use different models and use a variety of data preprocessing methods to achieve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
