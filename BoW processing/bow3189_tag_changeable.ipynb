{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import copy\n",
    "from nltk import Tree\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_type = ['NN', 'NNS', 'JJ', 'VB', 'VBD', 'VBP', 'VBZ', 'VBN', 'VBG', 'JJR', 'JJS']\n",
    "select_type = ['NN', 'NNS']\n",
    "letter_filter = re.compile('[^a-zA-Z]')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "useless_word = set(['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\\\n",
    "                  'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\\\n",
    "                  'z', 'as', 'an', 'and', 'are', 'were', 'at', 'by', 'down', 'for', 'from',\\\n",
    "                  'front', 'has', 'in', 'is', 'it', 'near', 'next', 'of', 'on', 'out',\\\n",
    "                  'over', 'several', 'sits', 'some', 'that', 'the', 'there', 'through',\\\n",
    "                  'to', 'top', 'up', 'while', 'with'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_descrption_files = [os.path.join('data/descriptions_train', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_tag_files = [os.path.join('data/tags_train', filename)\\\n",
    "                      for filename in os.listdir('data/tags_train')\\\n",
    "                      if filename.endswith('.txt')]\n",
    "test_descrption_files = [os.path.join('data/descriptions_test', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "training_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(a):\n",
    "    return [i for i in a if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_type(s, type_select):\n",
    "    dictionary = set()\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(s)]\n",
    "    sentences = [[letter_filter.sub('', sentences[i][j].lower())\\\n",
    "              for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
    "    sentences = [remove_empty(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for sent in sentences:\n",
    "        for e in sent:\n",
    "            if e[1] in type_select:\n",
    "                dictionary.add(snowball_stemmer.stem(e[0]))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_des_tag_vector():\n",
    "    selected = {}\n",
    "    for file in training_descrption_files:\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            nvj = find_type(sent, select_type)\n",
    "            for word in nvj:\n",
    "                if word in selected.keys():\n",
    "                    selected[word] += 1\n",
    "                else:\n",
    "                    selected[word] = 1\n",
    "    for file in training_tag_files:\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            validTag = letter_filter.sub('', sent.split(':')[1])\n",
    "            selected[validTag] = 100\n",
    "    \n",
    "    final = []\n",
    "    for k in selected.keys():\n",
    "        if selected[k] >= 2 and k not in final:\n",
    "            final.append(k)\n",
    "    \n",
    "    final_copy = copy.deepcopy(final)\n",
    "    for element in final:\n",
    "        if element in useless_word:\n",
    "            final_copy.remove(element)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTagFiles = [filename for filename in os.listdir('data/tags_train') if filename.endswith('.txt')]\n",
    "tags = set()\n",
    "for tagFile in allTagFiles:\n",
    "    f = open(os.path.join('data/tags_train',tagFile), 'r+')\n",
    "    for line in f:\n",
    "        tag = line.split(':')[1][:-1] if line.split(':')[1].endswith('\\n') else line.split(':')[1]\n",
    "        tags.add(tag)\n",
    "tagList = [tag for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = define_des_tag_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3031"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_files = [os.path.join('data/tags_train', filename) \\\n",
    "                             for filename in os.listdir('data/tags_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "test_tag_files = [os.path.join('data/tags_test', filename) \\\n",
    "                             for filename in os.listdir('data/tags_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "train_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_final = []\n",
    "for tag in tagList:\n",
    "    if not tag in a:\n",
    "        tags_final = tags_final + tag.split()\n",
    "    else:\n",
    "        tags_final.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_training_tag_weight(k):\n",
    "    Y_training_1 = []\n",
    "    for file in training_descrption_files:\n",
    "        vector_1 = dict.fromkeys(a, 0)\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                i = letter_filter.sub('', word.lower())\n",
    "                if i in a:\n",
    "                    vector_1[i] = 1\n",
    "        mm = [vector_1[element] for element in a]\n",
    "        Y_training_1.append(mm)\n",
    "\n",
    "    Y_training_2 = []\n",
    "    for file in training_descrption_files:\n",
    "        vector_2 = dict.fromkeys(tags_final, 0)\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                i = letter_filter.sub('', word.lower())\n",
    "                if i in tags_final:\n",
    "                    vector_2[i] = k\n",
    "        mm = [vector_2[element] for element in tags_final]\n",
    "        Y_training_2.append(mm)\n",
    "\n",
    "    Y_training = []\n",
    "    for i in range(10000):\n",
    "        combine = Y_training_1[i] + Y_training_2[i]\n",
    "        Y_training.append(combine)\n",
    "    \n",
    "    return Y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_test_tag_weight(k):\n",
    "    Y_test_1 = []\n",
    "    for file in test_descrption_files:\n",
    "        vector_1 = dict.fromkeys(a, 0)\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                i = letter_filter.sub('', word.lower())\n",
    "                if i in a:\n",
    "                    vector_1[i] = 1\n",
    "        mm = [vector_1[element] for element in a]\n",
    "        Y_test_1.append(mm)\n",
    "\n",
    "    Y_test_2 = []\n",
    "    for file in test_descrption_files:\n",
    "        vector_2 = dict.fromkeys(tags_final, 0)\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                i = letter_filter.sub('', word.lower())\n",
    "                if i in tags_final:\n",
    "                    vector_2[i] = k\n",
    "        mm = [vector_2[element] for element in tags_final]\n",
    "        Y_test_2.append(mm)\n",
    "\n",
    "\n",
    "    Y_test = []\n",
    "    for i in range(2000):\n",
    "        combine = Y_test_1[i] + Y_test_2[i]\n",
    "        Y_test.append(combine)\n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training = change_training_tag_weight(1)\n",
    "Y_test = change_test_tag_weight(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( Y_training, open( \"y_train_v7.p\", \"wb\" ) )\n",
    "pickle.dump( Y_test, open( \"y_test_v7.p\", \"wb\" ) )\n",
    "pickle.dump( ff, open( \"alll.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"\"\"\n",
    "    NP:    {<DT><WP><VBP>*<RB>*<VBN><IN><NN>}\n",
    "           {<NN|NNS|NNP|NNPS><IN    >*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS><CC>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS>+}\n",
    "    \"\"\"\n",
    "typer = nltk.RegexpParser(patterns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
