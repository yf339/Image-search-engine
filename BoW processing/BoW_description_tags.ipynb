{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import copy\n",
    "from nltk import Tree\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"\"\"\n",
    "    NP:    {<DT><WP><VBP>*<RB>*<VBN><IN><NN>}\n",
    "           {<NN|NNS|NNP|NNPS><IN    >*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS><CC>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS>+}\n",
    "    \"\"\"\n",
    "select_type = ['NN', 'NNS', 'VB', 'VBD', 'VBP', 'VBZ']\n",
    "typer = nltk.RegexpParser(patterns)\n",
    "letter_filter = re.compile('[^a-zA-Z]')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_descrption_files = [os.path.join('data/descriptions_train', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_tag_files = [os.path.join('data/tags_train', filename)\\\n",
    "                      for filename in os.listdir('data/tags_train')\\\n",
    "                      if filename.endswith('.txt')]\n",
    "test_descrption_files = [os.path.join('data/descriptions_test', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "training_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(a):\n",
    "    return [i for i in a if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_type(s, type_select):\n",
    "    dictionary = set()\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(s)]\n",
    "    sentences = [[letter_filter.sub('', sentences[i][j].lower())\\\n",
    "              for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
    "    sentences = [remove_empty(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for sent in sentences:\n",
    "        for e in sent:\n",
    "            if e[1] in type_select:\n",
    "                dictionary.add(lemmatizer.lemmatize(e[0]))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_des_tag_vector():\n",
    "    selected = {}\n",
    "    for file in training_descrption_files:\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            nvj = find_type(sent, select_type)\n",
    "            for word in nvj:\n",
    "                if word in selected.keys():\n",
    "                    selected[word] += 1\n",
    "                else:\n",
    "                    selected[word] = 1\n",
    "    for file in training_tag_files:\n",
    "        sentences = [line for line in open(file, 'r+')]\n",
    "        for sent in sentences:\n",
    "            validTag = letter_filter.sub('', sent.split(':')[1])\n",
    "            selected[validTag] = 100\n",
    "    \n",
    "    final = set()\n",
    "    for k in selected.keys():\n",
    "        if selected[k] >= 15 and selected[k] <= 2000:\n",
    "            final.add(k)\n",
    "    \n",
    "    final_copy = copy.deepcopy(final)\n",
    "    for element in final:\n",
    "        if element in tags_final:\n",
    "            final_copy.remove(element)\n",
    "    \n",
    "    return final_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = define_des_tag_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_final = []\n",
    "for tag in tagList:\n",
    "    if not tag in a:\n",
    "        tags_final = tags_final + tag.split()\n",
    "    else:\n",
    "        tags_final.append(tag)\n",
    "len(tags_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTagFiles = [filename for filename in os.listdir('data/tags_train') if filename.endswith('.txt')]\n",
    "tags = set()\n",
    "for tagFile in allTagFiles:\n",
    "    f = open(os.path.join('data/tags_train',tagFile), 'r+')\n",
    "    for line in f:\n",
    "        tag = line.split(':')[1][:-1] if line.split(':')[1].endswith('\\n') else line.split(':')[1]\n",
    "        tags.add(tag)\n",
    "tagList = [tag for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tags_test/0.txt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_files = [os.path.join('data/tags_train', filename) \\\n",
    "                             for filename in os.listdir('data/tags_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "test_tag_files = [os.path.join('data/tags_test', filename) \\\n",
    "                             for filename in os.listdir('data/tags_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "train_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_tag_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_tag_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training_1 = []\n",
    "for file in training_descrption_files:\n",
    "    vector_1 = dict.fromkeys(a, 0)\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            i = letter_filter.sub('', word.lower())\n",
    "            if i in a:\n",
    "                vector_1[i] += 1\n",
    "    \n",
    "    Y_training_1.append(list(vector_1.values()))\n",
    "\n",
    "Y_training_2 = []\n",
    "for tagFile in train_tag_files:\n",
    "    vector_2 = dict.fromkeys(tagList, 0)\n",
    "    f = open(os.path.join(tagFile), 'r+')\n",
    "    for line in f:\n",
    "        tag = line.split(':')[1][:-1] if line.split(':')[1].endswith('\\n') else line.split(':')[1]\n",
    "        if tag in tagList:\n",
    "            vector_2[tag] += 1\n",
    "    Y_training_2.append(list(vector_2.values()))\n",
    "\n",
    "Y_training = []\n",
    "for i in range(10000):\n",
    "    combine = Y_training_1[i] + Y_training_2[i]\n",
    "    Y_training.append(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_1 = []\n",
    "for file in test_descrption_files:\n",
    "    vector_1 = dict.fromkeys(a, 0)\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            i = letter_filter.sub('', word.lower())\n",
    "            if i in a:\n",
    "                vector_1[i] += 1\n",
    "    \n",
    "    Y_test_1.append(list(vector_1.values()))\n",
    "\n",
    "Y_test_2 = []\n",
    "for tagFile in test_tag_files:\n",
    "    vector_2 = dict.fromkeys(tagList, 0)\n",
    "    f = open(os.path.join(tagFile), 'r+')\n",
    "    for line in f:\n",
    "        tag = line.split(':')[1][:-1] if line.split(':')[1].endswith('\\n') else line.split(':')[1]\n",
    "        if tag in tagList:\n",
    "            vector_2[tag] += 1\n",
    "    Y_test_2.append(list(vector_2.values()))\n",
    "\n",
    "\n",
    "Y_test = []\n",
    "for i in range(2000):\n",
    "    combine = Y_test_1[i] + Y_test_2[i]\n",
    "    Y_test.append(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( Y_training, open( \"y_train_v2.p\", \"wb\" ) )\n",
    "pickle.dump( Y_test, open( \"y_test_v2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_type('she is dancing', select_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
