{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk import Tree\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"\"\"\n",
    "    NP:    {<DT><WP><VBP>*<RB>*<VBN><IN><NN>}\n",
    "           {<NN|NNS|NNP|NNPS><IN    >*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS><CC>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ>*<NN|NNS|NNP|NNPS>+}\n",
    "    \"\"\"\n",
    "\n",
    "typer = nltk.RegexpParser(patterns)\n",
    "letter_filter = re.compile('[^a-zA-Z]')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_descrption_files = [os.path.join('data/descriptions_train', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_train') \n",
    "                             if filename.endswith('.txt')]\n",
    "test_descrption_files = [os.path.join('data/descriptions_test', filename) \\\n",
    "                             for filename in os.listdir('data/descriptions_test') \n",
    "                             if filename.endswith('.txt')]\n",
    "training_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))\n",
    "test_descrption_files.sort(key=lambda x: int(x[x.rindex('/') + 1:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(a):\n",
    "    return [i for i in a if len(i) > 0]\n",
    "\n",
    "def find_type(s, type_select):\n",
    "    words = []\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(s)]\n",
    "    sentences = [[letter_filter.sub('', sentences[i][j].lower())\\\n",
    "              for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
    "    sentences = [remove_empty(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for sent in sentences:\n",
    "        for e in sent:\n",
    "            if e[1] in type_select:\n",
    "                words.append(lemmatizer.lemmatize(e[0]))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_noun = []\n",
    "for file in training_descrption_files:\n",
    "    new_noun = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        noun = find_type(sent, ['NN', 'NNS'])\n",
    "        new_noun = new_noun + noun\n",
    "    training_final_noun.append(new_noun)    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_adj = []\n",
    "for file in training_descrption_files:\n",
    "    new_adj = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        adj = find_type(sent, ['JJ'])\n",
    "        new_adj = new_adj + adj\n",
    "    training_final_adj.append(new_adj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_verb = []\n",
    "for file in training_descrption_files:\n",
    "    new_verb = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        verb = find_type(sent, ['VB', 'VBD', 'VBP', 'VBZ'])\n",
    "        new_verb = new_verb + verb\n",
    "    k = new_verb\n",
    "    for v in new_verb:\n",
    "        if v in ['ha', 'is', 'are', 'be', 'wa']:\n",
    "            k.remove(v)\n",
    "    training_final_verb.append(k)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_verb_adj = []\n",
    "for i in range(10000):\n",
    "    a = final_verb[i] + final_adj[i]\n",
    "    training_final_verb_adj.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_noun_verb = []\n",
    "for i in range(10000):\n",
    "    a = final_noun[i] + final_verb[i]\n",
    "    training_final_noun_verb.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_noun = []\n",
    "for file in test_descrption_files:\n",
    "    new_noun = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        noun = find_type(sent, ['NN', 'NNS'])\n",
    "        new_noun = new_noun + noun\n",
    "    test_final_noun.append(new_noun)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_adj = []\n",
    "for file in test_descrption_files:\n",
    "    new_adj = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        adj = find_type(sent, ['JJ'])\n",
    "        new_adj = new_adj + adj\n",
    "    test_final_adj.append(new_adj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_verb = []\n",
    "for file in test_descrption_files:\n",
    "    new_verb = []\n",
    "    sentences = [line for line in open(file, 'r+')]\n",
    "    for sent in sentences:\n",
    "        verb = find_type(sent, ['VB', 'VBD', 'VBP', 'VBZ'])\n",
    "        new_verb = new_verb + verb\n",
    "    k = new_verb\n",
    "    for v in new_verb:\n",
    "        if v in ['ha', 'is', 'are', 'be', 'wa']:\n",
    "            k.remove(v)\n",
    "    test_final_verb.append(k)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_verb_adj = []\n",
    "for i in range(2000):\n",
    "    a = test_final_verb[i] + test_final_adj[i]\n",
    "    test_final_verb_adj.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_noun_verb = []\n",
    "for i in range(2000):\n",
    "    a = test_final_noun[i] + test_final_verb[i]\n",
    "    test_final_noun_verb.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( training_final_noun, open( \"y_train_noun.p\", \"wb\" ) )\n",
    "pickle.dump( training_final_verb_adj, open( \"y_train_adj_verb.p\", \"wb\" ) )\n",
    "pickle.dump( test_final_noun, open( \"y_test_noun.p\", \"wb\" ) )\n",
    "pickle.dump( test_final_verb_adj, open( \"y_test_verb_adj.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( test_final_noun_verb, open( \"y_test_noun_verb.p\", \"wb\" ) )\n",
    "pickle.dump( training_final_noun_verb, open( \"y_train_noun_verb.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
